<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Doucx">
<meta name="dcterms.date" content="2022-12-23">
<meta name="description" content="torch.nn 模块">

<title>../../../blog - torch.nn</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="../../../blog - torch.nn">
<meta property="og:description" content="torch.nn 模块">
<meta property="og:site-name" content="../../../blog">
<meta name="twitter:title" content="../../../blog - torch.nn">
<meta name="twitter:description" content="torch.nn 模块">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="../../../blog/index.html" aria-current="page">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">torch.nn</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../index.html" class="sidebar-item-text sidebar-link">Doucx’s blog</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">blog</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../blog/index.html" class="sidebar-item-text sidebar-link">Doucx’s Blog</a>
  </div>
</li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">posts</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth2 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">12_23_torch.nn</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth3 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../blog/posts/22_12_23_torch.nn/12_23_torch.nn.html" class="sidebar-item-text sidebar-link active">torch.nn</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">blog</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">posts</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">01_06_unet噪音去除</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../blog/posts/23_01_06_unet噪音去除/01_06_unet噪音去除.html" class="sidebar-item-text sidebar-link">人声增强</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">blog</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false">posts</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">03_08_wcpdtoolbox</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../blog/posts/23_03_08_wcpdtoolbox/03_08_wcpdtoolbox.html" class="sidebar-item-text sidebar-link">WcpDToolBox</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">blog</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false">posts</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">03_09_模型的解释</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-12" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-12" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../blog/posts/23_03_09_模型的解释/03_09_模型的解释.html" class="sidebar-item-text sidebar-link">模型的解释</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="true">blog</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-13" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-13" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false">posts</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-14" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-14" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false">03_11_聊天机器人尝试</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-15" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-15" class="collapse list-unstyled sidebar-section depth3 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../../blog/posts/23_03_11_聊天机器人尝试/03_11_聊天机器人尝试.html" class="sidebar-item-text sidebar-link">从零(存疑)开始的聊天机器人</a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#torch.nn-note" id="toc-torch.nn-note" class="nav-link active" data-scroll-target="#torch.nn-note">torch.nn note</a></li>
  <li><a href="#containers容器" id="toc-containers容器" class="nav-link" data-scroll-target="#containers容器">Containers（容器）：</a>
  <ul class="collapse">
  <li><a href="#nn.module" id="toc-nn.module" class="nav-link" data-scroll-target="#nn.module">nn.Module</a></li>
  <li><a href="#add_modulename-module" id="toc-add_modulename-module" class="nav-link" data-scroll-target="#add_modulename-module">add_module(name, module)</a></li>
  <li><a href="#children" id="toc-children" class="nav-link" data-scroll-target="#children">children()</a></li>
  <li><a href="#cpudevice_idnone" id="toc-cpudevice_idnone" class="nav-link" data-scroll-target="#cpudevice_idnone">cpu(device_id=None)</a></li>
  <li><a href="#cudadevice_idnone" id="toc-cudadevice_idnone" class="nav-link" data-scroll-target="#cudadevice_idnone">cuda(device_id=None)</a></li>
  <li><a href="#double" id="toc-double" class="nav-link" data-scroll-target="#double">double()</a></li>
  <li><a href="#eval" id="toc-eval" class="nav-link" data-scroll-target="#eval">eval()</a></li>
  <li><a href="#float" id="toc-float" class="nav-link" data-scroll-target="#float">float()</a></li>
  <li><a href="#forward-input" id="toc-forward-input" class="nav-link" data-scroll-target="#forward-input">forward(* input)</a></li>
  <li><a href="#half" id="toc-half" class="nav-link" data-scroll-target="#half">half()</a></li>
  <li><a href="#load_state_dictstate_dict" id="toc-load_state_dictstate_dict" class="nav-link" data-scroll-target="#load_state_dictstate_dict">load_state_dict(state_dict)</a></li>
  <li><a href="#modules" id="toc-modules" class="nav-link" data-scroll-target="#modules">modules()</a></li>
  <li><a href="#named_children" id="toc-named_children" class="nav-link" data-scroll-target="#named_children">named_children()</a></li>
  <li><a href="#parametersmemonone" id="toc-parametersmemonone" class="nav-link" data-scroll-target="#parametersmemonone">parameters(memo=None)</a></li>
  <li><a href="#state_dictdestinationnone-prefixsource" id="toc-state_dictdestinationnone-prefixsource" class="nav-link" data-scroll-target="#state_dictdestinationnone-prefixsource">state_dict(destination=None, prefix=’’)[source]</a></li>
  <li><a href="#trainmodetrue" id="toc-trainmodetrue" class="nav-link" data-scroll-target="#trainmodetrue">train(mode=True)</a></li>
  <li><a href="#zero_grad" id="toc-zero_grad" class="nav-link" data-scroll-target="#zero_grad">zero_grad()</a></li>
  <li><a href="#class-torch.nn.sequential-args" id="toc-class-torch.nn.sequential-args" class="nav-link" data-scroll-target="#class-torch.nn.sequential-args">class torch.nn.Sequential(* args)</a></li>
  <li><a href="#class-torch.nn.modulelistmodulesnone" id="toc-class-torch.nn.modulelistmodulesnone" class="nav-link" data-scroll-target="#class-torch.nn.modulelistmodulesnone">class torch.nn.ModuleList(modules=None)</a></li>
  <li><a href="#appendmodule" id="toc-appendmodule" class="nav-link" data-scroll-target="#appendmodule">append(module)</a></li>
  <li><a href="#extendmodules" id="toc-extendmodules" class="nav-link" data-scroll-target="#extendmodules">extend(modules)</a></li>
  </ul></li>
  <li><a href="#卷积层" id="toc-卷积层" class="nav-link" data-scroll-target="#卷积层">卷积层</a>
  <ul class="collapse">
  <li><a href="#class-torch.nn.conv1d" id="toc-class-torch.nn.conv1d" class="nav-link" data-scroll-target="#class-torch.nn.conv1d">class torch.nn.Conv1d</a></li>
  <li><a href="#class-torch.nn.convtranspose1d" id="toc-class-torch.nn.convtranspose1d" class="nav-link" data-scroll-target="#class-torch.nn.convtranspose1d">class torch.nn.ConvTranspose1d</a></li>
  </ul></li>
  <li><a href="#池化层" id="toc-池化层" class="nav-link" data-scroll-target="#池化层">池化层</a>
  <ul class="collapse">
  <li><a href="#nn.maxpool1d" id="toc-nn.maxpool1d" class="nav-link" data-scroll-target="#nn.maxpool1d">nn.MaxPool1d</a></li>
  <li><a href="#nn.avgpool1d" id="toc-nn.avgpool1d" class="nav-link" data-scroll-target="#nn.avgpool1d">nn.AvgPool1d</a></li>
  <li><a href="#nn.fractionalmaxpool2d" id="toc-nn.fractionalmaxpool2d" class="nav-link" data-scroll-target="#nn.fractionalmaxpool2d">nn.FractionalMaxPool2d</a></li>
  <li><a href="#nn.lppool2d" id="toc-nn.lppool2d" class="nav-link" data-scroll-target="#nn.lppool2d">nn.LPPool2d</a></li>
  <li><a href="#nn.adaptivemaxpool2d" id="toc-nn.adaptivemaxpool2d" class="nav-link" data-scroll-target="#nn.adaptivemaxpool2d">nn.AdaptiveMaxPool2d</a></li>
  </ul></li>
  <li><a href="#non-linear-activations-非线性激活函数" id="toc-non-linear-activations-非线性激活函数" class="nav-link" data-scroll-target="#non-linear-activations-非线性激活函数">Non-Linear Activations 非线性激活函数</a>
  <ul class="collapse">
  <li><a href="#torch.nn.relu" id="toc-torch.nn.relu" class="nav-link" data-scroll-target="#torch.nn.relu">torch.nn.ReLU</a></li>
  <li><a href="#nn.relu6" id="toc-nn.relu6" class="nav-link" data-scroll-target="#nn.relu6">nn.ReLU6</a></li>
  <li><a href="#nn.elu" id="toc-nn.elu" class="nav-link" data-scroll-target="#nn.elu">nn.ELU</a></li>
  <li><a href="#nn.prelu" id="toc-nn.prelu" class="nav-link" data-scroll-target="#nn.prelu">nn.PReLU</a></li>
  <li><a href="#nn.leakyrelu" id="toc-nn.leakyrelu" class="nav-link" data-scroll-target="#nn.leakyrelu">nn.LeakyReLU</a></li>
  <li><a href="#nn.threshold" id="toc-nn.threshold" class="nav-link" data-scroll-target="#nn.threshold">nn.Threshold</a></li>
  <li><a href="#nn.hardtanh" id="toc-nn.hardtanh" class="nav-link" data-scroll-target="#nn.hardtanh">nn.Hardtanh</a></li>
  <li><a href="#nn.sigmoid" id="toc-nn.sigmoid" class="nav-link" data-scroll-target="#nn.sigmoid">nn.Sigmoid</a></li>
  <li><a href="#nn.tanh" id="toc-nn.tanh" class="nav-link" data-scroll-target="#nn.tanh">nn.Tanh</a></li>
  <li><a href="#nn.logsigmoid" id="toc-nn.logsigmoid" class="nav-link" data-scroll-target="#nn.logsigmoid">nn.LogSigmoid</a></li>
  <li><a href="#nn.softplus" id="toc-nn.softplus" class="nav-link" data-scroll-target="#nn.softplus">nn.Softplus</a></li>
  <li><a href="#nn.softshrink" id="toc-nn.softshrink" class="nav-link" data-scroll-target="#nn.softshrink">nn.Softshrink</a></li>
  <li><a href="#nn.softsign" id="toc-nn.softsign" class="nav-link" data-scroll-target="#nn.softsign">nn.Softsign</a></li>
  <li><a href="#nn.tanhshrink" id="toc-nn.tanhshrink" class="nav-link" data-scroll-target="#nn.tanhshrink">nn.Tanhshrink</a></li>
  <li><a href="#nn.softmin" id="toc-nn.softmin" class="nav-link" data-scroll-target="#nn.softmin">nn.Softmin</a></li>
  <li><a href="#nn.softmax" id="toc-nn.softmax" class="nav-link" data-scroll-target="#nn.softmax">nn.Softmax</a></li>
  <li><a href="#nn.logsoftmax" id="toc-nn.logsoftmax" class="nav-link" data-scroll-target="#nn.logsoftmax">nn.LogSoftmax</a></li>
  </ul></li>
  <li><a href="#normalization-layers-标准化层" id="toc-normalization-layers-标准化层" class="nav-link" data-scroll-target="#normalization-layers-标准化层">Normalization layers 标准化层</a>
  <ul class="collapse">
  <li><a href="#nn.batchnorm2d" id="toc-nn.batchnorm2d" class="nav-link" data-scroll-target="#nn.batchnorm2d">nn.BatchNorm2d</a></li>
  </ul></li>
  <li><a href="#recurrent-layers-循环层" id="toc-recurrent-layers-循环层" class="nav-link" data-scroll-target="#recurrent-layers-循环层">Recurrent layers 循环层</a>
  <ul class="collapse">
  <li><a href="#nn.rnnargs-kwargs" id="toc-nn.rnnargs-kwargs" class="nav-link" data-scroll-target="#nn.rnnargs-kwargs">nn.RNN(args, *kwargs)</a>
  <ul class="collapse">
  <li><a href="#参数" id="toc-参数" class="nav-link" data-scroll-target="#参数">参数</a></li>
  <li><a href="#输入-input-h_0" id="toc-输入-input-h_0" class="nav-link" data-scroll-target="#输入-input-h_0">输入 (input, h_0)</a></li>
  <li><a href="#输出-output-h_n" id="toc-输出-output-h_n" class="nav-link" data-scroll-target="#输出-output-h_n">输出 (output, h_n)</a></li>
  <li><a href="#属性" id="toc-属性" class="nav-link" data-scroll-target="#属性">属性</a></li>
  <li><a href="#示例" id="toc-示例" class="nav-link" data-scroll-target="#示例">示例</a></li>
  </ul></li>
  <li><a href="#nn.lstm" id="toc-nn.lstm" class="nav-link" data-scroll-target="#nn.lstm">nn.LSTM</a>
  <ul class="collapse">
  <li><a href="#参数-1" id="toc-参数-1" class="nav-link" data-scroll-target="#参数-1">参数</a></li>
  <li><a href="#输入-input-h_0-c_0" id="toc-输入-input-h_0-c_0" class="nav-link" data-scroll-target="#输入-input-h_0-c_0">输入: input, (h_0, c_0)</a></li>
  <li><a href="#输出output-h_n-c_n" id="toc-输出output-h_n-c_n" class="nav-link" data-scroll-target="#输出output-h_n-c_n">输出:output, (h_n, c_n)</a></li>
  <li><a href="#属性-1" id="toc-属性-1" class="nav-link" data-scroll-target="#属性-1">属性</a></li>
  </ul></li>
  <li><a href="#nn.gru" id="toc-nn.gru" class="nav-link" data-scroll-target="#nn.gru">nn.GRU</a>
  <ul class="collapse">
  <li><a href="#示例-1" id="toc-示例-1" class="nav-link" data-scroll-target="#示例-1">示例：</a></li>
  </ul></li>
  <li><a href="#nn.rnncell" id="toc-nn.rnncell" class="nav-link" data-scroll-target="#nn.rnncell">nn.RNNCell</a>
  <ul class="collapse">
  <li><a href="#参数-2" id="toc-参数-2" class="nav-link" data-scroll-target="#参数-2">参数</a></li>
  <li><a href="#输入-input-hidden" id="toc-输入-input-hidden" class="nav-link" data-scroll-target="#输入-input-hidden">输入： input, hidden</a></li>
  <li><a href="#输出-h" id="toc-输出-h" class="nav-link" data-scroll-target="#输出-h">输出： h’</a></li>
  <li><a href="#属性-2" id="toc-属性-2" class="nav-link" data-scroll-target="#属性-2">属性：</a></li>
  <li><a href="#示例-2" id="toc-示例-2" class="nav-link" data-scroll-target="#示例-2">示例</a></li>
  </ul></li>
  <li><a href="#nn.lstmcell" id="toc-nn.lstmcell" class="nav-link" data-scroll-target="#nn.lstmcell">nn.LSTMCell</a>
  <ul class="collapse">
  <li><a href="#参数-3" id="toc-参数-3" class="nav-link" data-scroll-target="#参数-3">参数</a></li>
  <li><a href="#输入-input-h_0-c_0-1" id="toc-输入-input-h_0-c_0-1" class="nav-link" data-scroll-target="#输入-input-h_0-c_0-1">输入: input, (h_0, c_0)</a></li>
  <li><a href="#输出-h_1-c_1" id="toc-输出-h_1-c_1" class="nav-link" data-scroll-target="#输出-h_1-c_1">输出： h_1, c_1</a></li>
  <li><a href="#属性-3" id="toc-属性-3" class="nav-link" data-scroll-target="#属性-3">属性:</a></li>
  <li><a href="#例子" id="toc-例子" class="nav-link" data-scroll-target="#例子">例子</a></li>
  </ul></li>
  <li><a href="#nn.grucell" id="toc-nn.grucell" class="nav-link" data-scroll-target="#nn.grucell">nn.GRUCell</a></li>
  </ul></li>
  <li><a href="#linear-layers-线性层" id="toc-linear-layers-线性层" class="nav-link" data-scroll-target="#linear-layers-线性层">Linear layers 线性层</a>
  <ul class="collapse">
  <li><a href="#nn.linearin_features-out_features-biastrue" id="toc-nn.linearin_features-out_features-biastrue" class="nav-link" data-scroll-target="#nn.linearin_features-out_features-biastrue">nn.Linear(in_features, out_features, bias=True)</a>
  <ul class="collapse">
  <li><a href="#参数-4" id="toc-参数-4" class="nav-link" data-scroll-target="#参数-4">参数：</a></li>
  <li><a href="#形状" id="toc-形状" class="nav-link" data-scroll-target="#形状">形状：</a></li>
  <li><a href="#属性-4" id="toc-属性-4" class="nav-link" data-scroll-target="#属性-4">属性：</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#dropout-layers-丢弃层" id="toc-dropout-layers-丢弃层" class="nav-link" data-scroll-target="#dropout-layers-丢弃层">Dropout layers 丢弃层</a>
  <ul class="collapse">
  <li><a href="#nn.dropout-nn.dropout2d3d" id="toc-nn.dropout-nn.dropout2d3d" class="nav-link" data-scroll-target="#nn.dropout-nn.dropout2d3d">nn.Dropout / nn.Dropout2d,3d</a>
  <ul class="collapse">
  <li><a href="#参数-5" id="toc-参数-5" class="nav-link" data-scroll-target="#参数-5">参数：</a></li>
  <li><a href="#形状-1" id="toc-形状-1" class="nav-link" data-scroll-target="#形状-1">形状：</a></li>
  <li><a href="#例子-1" id="toc-例子-1" class="nav-link" data-scroll-target="#例子-1">例子：</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sparse-layers-稀疏层词向量产生层" id="toc-sparse-layers-稀疏层词向量产生层" class="nav-link" data-scroll-target="#sparse-layers-稀疏层词向量产生层">Sparse layers 稀疏层（？）词向量产生层（！）</a>
  <ul class="collapse">
  <li><a href="#nn.embedding" id="toc-nn.embedding" class="nav-link" data-scroll-target="#nn.embedding">nn.Embedding</a></li>
  <li><a href="#参数-6" id="toc-参数-6" class="nav-link" data-scroll-target="#参数-6">参数：</a></li>
  <li><a href="#属性-5" id="toc-属性-5" class="nav-link" data-scroll-target="#属性-5">属性：</a></li>
  <li><a href="#输入-longtensor-n-w" id="toc-输入-longtensor-n-w" class="nav-link" data-scroll-target="#输入-longtensor-n-w">输入： LongTensor (N, W),</a></li>
  <li><a href="#输出-n-w-embedding_dim" id="toc-输出-n-w-embedding_dim" class="nav-link" data-scroll-target="#输出-n-w-embedding_dim">输出： (N, W, embedding_dim)</a></li>
  <li><a href="#示例-3" id="toc-示例-3" class="nav-link" data-scroll-target="#示例-3">示例</a></li>
  </ul></li>
  <li><a href="#distance-functions" id="toc-distance-functions" class="nav-link" data-scroll-target="#distance-functions">Distance functions</a>
  <ul class="collapse">
  <li><a href="#nn.pairwisedistancep2-eps1e-06" id="toc-nn.pairwisedistancep2-eps1e-06" class="nav-link" data-scroll-target="#nn.pairwisedistancep2-eps1e-06">nn.PairwiseDistance(p=2, eps=1e-06)</a>
  <ul class="collapse">
  <li><a href="#参数-7" id="toc-参数-7" class="nav-link" data-scroll-target="#参数-7">参数：</a></li>
  <li><a href="#输入-nd其中d向量维数" id="toc-输入-nd其中d向量维数" class="nav-link" data-scroll-target="#输入-nd其中d向量维数">输入： (N,D)，其中D=向量维数</a></li>
  <li><a href="#输出-n1" id="toc-输出-n1" class="nav-link" data-scroll-target="#输出-n1">输出： (N,1)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#loss-functions-损失函数" id="toc-loss-functions-损失函数" class="nav-link" data-scroll-target="#loss-functions-损失函数">Loss functions 损失函数</a>
  <ul class="collapse">
  <li><a href="#nn.l1loss" id="toc-nn.l1loss" class="nav-link" data-scroll-target="#nn.l1loss">nn.L1Loss</a></li>
  <li><a href="#nn.mseloss" id="toc-nn.mseloss" class="nav-link" data-scroll-target="#nn.mseloss">nn.MSELoss</a></li>
  <li><a href="#nn.crossentropyloss" id="toc-nn.crossentropyloss" class="nav-link" data-scroll-target="#nn.crossentropyloss">nn.CrossEntropyLoss</a>
  <ul class="collapse">
  <li><a href="#调用时参数" id="toc-调用时参数" class="nav-link" data-scroll-target="#调用时参数">调用时参数：</a></li>
  <li><a href="#形状shape" id="toc-形状shape" class="nav-link" data-scroll-target="#形状shape">形状(shape)：</a></li>
  </ul></li>
  <li><a href="#nn.nllloss" id="toc-nn.nllloss" class="nav-link" data-scroll-target="#nn.nllloss">nn.NLLLoss</a>
  <ul class="collapse">
  <li><a href="#参数-8" id="toc-参数-8" class="nav-link" data-scroll-target="#参数-8">参数：</a></li>
  <li><a href="#形状-2" id="toc-形状-2" class="nav-link" data-scroll-target="#形状-2">形状:</a></li>
  <li><a href="#例子-2" id="toc-例子-2" class="nav-link" data-scroll-target="#例子-2">例子：</a></li>
  <li><a href="#nn.nllloss2d" id="toc-nn.nllloss2d" class="nav-link" data-scroll-target="#nn.nllloss2d">nn.NLLLoss2d</a></li>
  <li><a href="#nn.kldivloss" id="toc-nn.kldivloss" class="nav-link" data-scroll-target="#nn.kldivloss">nn.KLDivLoss</a></li>
  </ul></li>
  <li><a href="#nn.bceloss" id="toc-nn.bceloss" class="nav-link" data-scroll-target="#nn.bceloss">nn.BCELoss</a></li>
  <li><a href="#nn.marginrankingloss" id="toc-nn.marginrankingloss" class="nav-link" data-scroll-target="#nn.marginrankingloss">nn.MarginRankingLoss</a></li>
  <li><a href="#nn.hingeembeddingloss" id="toc-nn.hingeembeddingloss" class="nav-link" data-scroll-target="#nn.hingeembeddingloss">nn.HingeEmbeddingLoss</a>
  <ul class="collapse">
  <li><a href="#例子-3" id="toc-例子-3" class="nav-link" data-scroll-target="#例子-3">例子：</a></li>
  </ul></li>
  <li><a href="#nn.multilabelmarginloss" id="toc-nn.multilabelmarginloss" class="nav-link" data-scroll-target="#nn.multilabelmarginloss">nn.MultiLabelMarginLoss</a>
  <ul class="collapse">
  <li><a href="#例子-4" id="toc-例子-4" class="nav-link" data-scroll-target="#例子-4">例子：</a></li>
  </ul></li>
  <li><a href="#nn.smoothl1loss" id="toc-nn.smoothl1loss" class="nav-link" data-scroll-target="#nn.smoothl1loss">nn.SmoothL1Loss</a></li>
  <li><a href="#nn.softmarginloss" id="toc-nn.softmarginloss" class="nav-link" data-scroll-target="#nn.softmarginloss">nn.SoftMarginLoss</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/doucx/blog/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">torch.nn</h1>
</div>

<div>
  <div class="description">
    torch.nn 模块
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Doucx </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 23, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> kaggle</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> zipfile</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastkaggle <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastcore.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.autograd <span class="im">import</span> Variable</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> autograd</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#hide</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>nbdev_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="torch.nn-note" class="level1">
<h1>torch.nn note</h1>
</section>
<section id="containers容器" class="level1">
<h1>Containers（容器）：</h1>
<section id="nn.module" class="level2">
<h2 class="anchored" data-anchor-id="nn.module">nn.Module</h2>
<p>一个基本的卷积网络，包含初始化和forward</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">5</span>)  <span class="co"># submodule: Conv2d</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">5</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.relu(<span class="va">self</span>.conv2(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>可见，其中包含： - <code>__init__</code> 模型初始化方法 - <code>super(Model, self).__init__()</code> 初始化上一层 - <code>forward</code> 前向传播</p>
</section>
<section id="add_modulename-module" class="level2">
<h2 class="anchored" data-anchor-id="add_modulename-module">add_module(name, module)</h2>
<p>将一个 <code>child module</code> 添加到当前 <code>modle</code>。<br>
被添加的<code>module</code>可以通过 <code>name</code>属性来获取。 例：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_module(<span class="st">"conv"</span>, nn.Conv2d(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># self.conv = nn.Conv2d(10, 20, 4) 和上面这个增加module的方式等价</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.conv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</code></pre>
</div>
</div>
</section>
<section id="children" class="level2">
<h2 class="anchored" data-anchor-id="children">children()</h2>
<p>返回模型子模块的迭代器</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet18()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sub_module <span class="kw">in</span> model.children():</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(sub_module)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">4</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
ReLU(inplace=True)
MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)</code></pre>
</div>
</div>
</section>
<section id="cpudevice_idnone" class="level2">
<h2 class="anchored" data-anchor-id="cpudevice_idnone">cpu(device_id=None)</h2>
<p>将所有的模型参数(parameters)和buffers复制到CPU</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.cpu()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="cudadevice_idnone" class="level2">
<h2 class="anchored" data-anchor-id="cudadevice_idnone">cuda(device_id=None)</h2>
<p>将所有的模型参数(parameters)和buffers赋值GPU</p>
<p>参数说明:</p>
<p>device_id (int, optional) – 如果指定的话，所有的模型参数都会复制到指定的设备上。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.cuda()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="double" class="level2">
<h2 class="anchored" data-anchor-id="double">double()</h2>
<p>将parameters和buffers的数据类型转换成double。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.double()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="eval" class="level2">
<h2 class="anchored" data-anchor-id="eval">eval()</h2>
<p>将模型设置成evaluation模式</p>
<p>仅仅当模型中有Dropout和BatchNorm是才会有影响。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>若是在模型的非训练阶段（如 evaluation 阶段）未使用 model.eval() 将 model 设置成评估模式，有可能会造成同一样本的多次推断结果不一致的情况</p>
</section>
<section id="float" class="level2">
<h2 class="anchored" data-anchor-id="float">float()</h2>
<p>将parameters和buffers的数据类型转换成float。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.<span class="bu">float</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="forward-input" class="level2">
<h2 class="anchored" data-anchor-id="forward-input">forward(* input)</h2>
<p>定义了每次执行的 计算步骤。 在所有的子类中都需要重写这个函数。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">5</span>)  <span class="co"># submodule: Conv2d</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">5</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.relu(<span class="va">self</span>.conv2(x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>就是这个forward</p>
</section>
<section id="half" class="level2">
<h2 class="anchored" data-anchor-id="half">half()</h2>
<p>将parameters和buffers的数据类型转换成half。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.half()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="load_state_dictstate_dict" class="level2">
<h2 class="anchored" data-anchor-id="load_state_dictstate_dict">load_state_dict(state_dict)</h2>
<p>将state_dict中的parameters和buffers复制到此module和它的后代中。state_dict中的key<strong>必须</strong>和 model.state_dict()返回的key一致。 NOTE：用来加载模型参数。</p>
<p>参数说明:</p>
<p>state_dict (dict) – 保存parameters和persistent buffers的字典。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet18()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(resnet18().state_dict())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;All keys matched successfully&gt;</code></pre>
</div>
</div>
</section>
<section id="modules" class="level2">
<h2 class="anchored" data-anchor-id="modules">modules()</h2>
<p>返回一个包含 当前模型 所有模块的迭代器。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_module(<span class="st">"conv"</span>, nn.Conv2d(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">4</span>))</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_module(<span class="st">"conv1"</span>, nn.Conv2d(<span class="dv">20</span>, <span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> module <span class="kw">in</span> model.modules():</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(module)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'-------------CUTTHERE---------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model(
  (conv): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (conv1): Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
)
-------------CUTTHERE---------------
Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
-------------CUTTHERE---------------
Conv2d(20, 10, kernel_size=(4, 4), stride=(1, 1))
-------------CUTTHERE---------------</code></pre>
</div>
</div>
<p>可以看见，它输出了所有的模块</p>
<p>重复的模块只被返回一次(children()也是)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        submodule <span class="op">=</span> nn.Conv2d(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">4</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_module(<span class="st">'conv1'</span>, submodule)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_module(<span class="st">'cv2'</span>, submodule)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.add_module(<span class="st">'cv20'</span>, submodule)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> module <span class="kw">in</span> model.modules():</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(module)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'-------------CUTTHERE---------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model(
  (conv1): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (cv2): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
  (cv20): Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
)
-------------CUTTHERE---------------
Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))
-------------CUTTHERE---------------</code></pre>
</div>
</div>
</section>
<section id="named_children" class="level2">
<h2 class="anchored" data-anchor-id="named_children">named_children()</h2>
<p>返回 包含 模型当前子模块 的迭代器，yield 模块名字和模块本身。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, module <span class="kw">in</span> model.named_children():</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># if name in ['conv4', 'conv5']:</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(name, module)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>conv1 Conv2d(10, 20, kernel_size=(4, 4), stride=(1, 1))</code></pre>
</div>
</div>
</section>
<section id="parametersmemonone" class="level2">
<h2 class="anchored" data-anchor-id="parametersmemonone">parameters(memo=None)</h2>
<p>返回一个 包含模型所有参数 的迭代器。</p>
<p>一般用来当作<code>optimizer</code>的参数。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> model.parameters():</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">type</span>(i.data), i.size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'torch.Tensor'&gt; torch.Size([20, 10, 4, 4])
&lt;class 'torch.Tensor'&gt; torch.Size([20])</code></pre>
</div>
</div>
</section>
<section id="state_dictdestinationnone-prefixsource" class="level2">
<h2 class="anchored" data-anchor-id="state_dictdestinationnone-prefixsource">state_dict(destination=None, prefix=’’)[source]</h2>
<p>返回一个字典，保存着module的所有状态（state）。</p>
<p>parameters和persistent buffers都会包含在字典中，字典的key就是parameter和buffer的 names。</p>
<p>例子：</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Linear(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vari <span class="op">=</span> Variable(torch.rand([<span class="dv">1</span>]))</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.par <span class="op">=</span> nn.Parameter(torch.rand([<span class="dv">1</span>]))</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"buffer"</span>, torch.randn([<span class="dv">2</span>, <span class="dv">3</span>]))</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model()</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.state_dict())  <span class="co"># .keys())</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OrderedDict([('par', tensor([0.4542])), ('buffer', tensor([[ 2.1136,  0.1283,  0.4152],
        [ 1.4492, -0.8829,  0.0859]])), ('conv2.weight', tensor([[-0.0936],
        [ 0.7408]])), ('conv2.bias', tensor([-0.8880, -0.3095]))])</code></pre>
</div>
</div>
<p>NOTE：<code>.keys()</code>:显示<code>OrderedDict</code>的key</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model.state_dict().keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>odict_keys(['par', 'buffer', 'conv2.weight', 'conv2.bias'])</code></pre>
</div>
</div>
</section>
<section id="trainmodetrue" class="level2">
<h2 class="anchored" data-anchor-id="trainmodetrue">train(mode=True)</h2>
<p>将module设置为 training mode。</p>
<p>仅仅当模型中有Dropout和BatchNorm是才会有影响。</p>
</section>
<section id="zero_grad" class="level2">
<h2 class="anchored" data-anchor-id="zero_grad">zero_grad()</h2>
<p>将module中的所有模型参数的梯度设置为0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>model.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="class-torch.nn.sequential-args" class="level2">
<h2 class="anchored" data-anchor-id="class-torch.nn.sequential-args">class torch.nn.Sequential(* args)</h2>
<p>一个时序容器。<code>Modules</code> 会以他们传入的顺序被添加到容器中。当然，也可以传入一个<code>OrderedDict</code>。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">5</span>),</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    nn.Conv2d(<span class="dv">20</span>, <span class="dv">64</span>, <span class="dv">5</span>),</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    nn.ReLU()</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of using Sequential with OrderedDict</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(OrderedDict([</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'conv1'</span>, nn.Conv2d(<span class="dv">1</span>, <span class="dv">20</span>, <span class="dv">5</span>)),</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'relu1'</span>, nn.ReLU()),</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'conv2'</span>, nn.Conv2d(<span class="dv">20</span>, <span class="dv">64</span>, <span class="dv">5</span>)),</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'relu2'</span>, nn.ReLU())</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Sequential(
  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))
  (relu1): ReLU()
  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))
  (relu2): ReLU()
)</code></pre>
</div>
</div>
</section>
<section id="class-torch.nn.modulelistmodulesnone" class="level2">
<h2 class="anchored" data-anchor-id="class-torch.nn.modulelistmodulesnone">class torch.nn.ModuleList(modules=None)</h2>
<p>将<code>submodules</code>保存在一个<code>list</code>中。</p>
<p><code>ModuleList</code>可以像一般的Python <code>list</code>一样被索引。而且<code>ModuleList</code>中包含的<code>modules</code>已经被正确的注册，对所有的<code>module method</code>可见。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyModule(nn.Module):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MyModule, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linears <span class="op">=</span> nn.ModuleList([nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)])</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ModuleList can act as an iterable, or be indexed         using ints</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, l <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.linears):</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.linears[i <span class="op">//</span> <span class="dv">2</span>](x) <span class="op">+</span> l(x)</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MyModule()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> module <span class="kw">in</span> model.modules():</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(module)</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'-------------CUTTHERE---------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MyModule(
  (linears): ModuleList(
    (0): Linear(in_features=10, out_features=10, bias=True)
    (1): Linear(in_features=10, out_features=10, bias=True)
    (2): Linear(in_features=10, out_features=10, bias=True)
    (3): Linear(in_features=10, out_features=10, bias=True)
    (4): Linear(in_features=10, out_features=10, bias=True)
    (5): Linear(in_features=10, out_features=10, bias=True)
    (6): Linear(in_features=10, out_features=10, bias=True)
    (7): Linear(in_features=10, out_features=10, bias=True)
    (8): Linear(in_features=10, out_features=10, bias=True)
    (9): Linear(in_features=10, out_features=10, bias=True)
  )
)
-------------CUTTHERE---------------
ModuleList(
  (0): Linear(in_features=10, out_features=10, bias=True)
  (1): Linear(in_features=10, out_features=10, bias=True)
  (2): Linear(in_features=10, out_features=10, bias=True)
  (3): Linear(in_features=10, out_features=10, bias=True)
  (4): Linear(in_features=10, out_features=10, bias=True)
  (5): Linear(in_features=10, out_features=10, bias=True)
  (6): Linear(in_features=10, out_features=10, bias=True)
  (7): Linear(in_features=10, out_features=10, bias=True)
  (8): Linear(in_features=10, out_features=10, bias=True)
  (9): Linear(in_features=10, out_features=10, bias=True)
)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------
Linear(in_features=10, out_features=10, bias=True)
-------------CUTTHERE---------------</code></pre>
</div>
</div>
</section>
<section id="appendmodule" class="level2">
<h2 class="anchored" data-anchor-id="appendmodule">append(module)</h2>
<p>等价于 <code>list</code> 的 <code>append()</code></p>
<p>参数说明:</p>
<p>module (nn.Module) – 要 append 的module</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>list0.append(nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>ModuleList(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): Linear(in_features=1, out_features=10, bias=True)
  (2): Linear(in_features=1, out_features=10, bias=True)
  (3): Linear(in_features=1, out_features=10, bias=True)
  (4): Linear(in_features=1, out_features=10, bias=True)
  (5): Linear(in_features=1, out_features=10, bias=True)
  (6): Linear(in_features=1, out_features=10, bias=True)
  (7): Linear(in_features=1, out_features=10, bias=True)
  (8): Linear(in_features=1, out_features=10, bias=True)
)</code></pre>
</div>
</div>
</section>
<section id="extendmodules" class="level2">
<h2 class="anchored" data-anchor-id="extendmodules">extend(modules)</h2>
<p>等价于 <code>list</code> 的 <code>extend()</code> 方法</p>
<p>参数说明:</p>
<p>modules (list) – list of modules to append</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>list0 <span class="op">=</span> nn.ModuleList([nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>list0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>ModuleList(
  (0): Linear(in_features=1, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>list0.extend([nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>), nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>)])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>ModuleList(
  (0): Linear(in_features=1, out_features=10, bias=True)
  (1): Linear(in_features=1, out_features=10, bias=True)
  (2): Linear(in_features=1, out_features=10, bias=True)
)</code></pre>
</div>
</div>
<p>extend输入迭代器/列表等，append输入一个数据</p>
</section>
</section>
<section id="卷积层" class="level1">
<h1>卷积层</h1>
<section id="class-torch.nn.conv1d" class="level2">
<h2 class="anchored" data-anchor-id="class-torch.nn.conv1d">class torch.nn.Conv1d</h2>
<ul>
<li>in_channels(int)
<ul>
<li>输入信号的通道(比如RGB)</li>
</ul></li>
<li>out_channels(int)
<ul>
<li>卷积产生的通道（比如六个通道）</li>
</ul></li>
<li>kerner_size(int or tuple)
<ul>
<li>卷积核的尺寸</li>
</ul></li>
<li>stride(int or tuple, optional)
<ul>
<li>卷积步长（用来节省计算量）</li>
</ul></li>
<li>padding (int or tuple, optional)
<ul>
<li>输入的每一条边补充0的层数（补充边缘）</li>
</ul></li>
<li>dilation(int or tuple, `optional``)
<ul>
<li>卷积核元素之间的间距（空洞卷积）</li>
</ul></li>
<li>groups(int, optional)
<ul>
<li>从输入通道到输出通道的阻塞连接数（控制输入和输出之间的连接， group=1，输出是所有的输入的卷积；group=2，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。）</li>
</ul></li>
<li>bias(bool, optional) - 如果bias=True，添加偏置</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.Conv1d(<span class="dv">16</span>, <span class="dv">33</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.randn(<span class="dv">1</span>, <span class="dv">16</span>, <span class="dv">3</span>))</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> m(input0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>注意：对于一组图片，会产生一组输出</p>
<p>图片叠加在第一维度</p>
<p>比如<code>torch.randn(10, 16, 3)</code>在这里相当于十张大小为长3通道16的图片</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>output.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 33, 1])</code></pre>
</div>
</div>
<p><code>nn.Conv2d,3d</code>同理，只有卷积核尺寸不同</p>
</section>
<section id="class-torch.nn.convtranspose1d" class="level2">
<h2 class="anchored" data-anchor-id="class-torch.nn.convtranspose1d">class torch.nn.ConvTranspose1d</h2>
<p>1维的解卷积（转置卷积）操作（transposed convolution operator，注意改视作操作可视作解卷积操作，但并不是真正的解卷积操作） 该模块可以看作是Conv1d相对于其输入的梯度，有时（但不正确地）被称为解卷积操作。</p>
<p>很显然，它会丢失信息，并且会产生棋盘格伪影</p>
<p>输入：<br>
class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)</p>
<ul>
<li>in_channels(int) – 输入信号的通道数</li>
<li>out_channels(int) – 卷积产生的通道</li>
<li>kernel_size(int or tuple) - 卷积核的大小</li>
<li>stride(int or tuple, optional) - 卷积步长</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
<li>output_padding(int or tuple, optional) - 输出的每一条边补充0的层数</li>
<li>dilation(int or tuple, optional) – 卷积核元素之间的间距</li>
<li>groups(int, optional) – 从输入通道到输出通道的阻塞连接数</li>
<li>bias(bool, optional) - 如果bias=True，添加偏置</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> PILImage.create(imgpath[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> untar_data(URLs.PETS)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>imgpath <span class="op">=</span> get_image_files(data<span class="op">/</span><span class="st">'images'</span>)</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> ToTensor()(PILImage.create(imgpath[<span class="dv">0</span>]))</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> img.<span class="bu">float</span>()</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> img.unsqueeze(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>创建一个卷积核</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>imgc <span class="op">=</span> conv(img)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>imga <span class="op">=</span> array(imgc.detach())</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(imga[<span class="dv">0</span>][<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>&lt;matplotlib.image.AxesImage&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-42-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>转置卷积</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>convt <span class="op">=</span> nn.ConvTranspose2d(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>imgc.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 1, 187, 249])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>imgc[<span class="dv">0</span>].size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 187, 249])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>imgt <span class="op">=</span> convt(imgc[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>imgt <span class="op">=</span> imgt<span class="op">*</span><span class="dv">255</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>imgt <span class="op">=</span> imgt.<span class="bu">int</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>imgt <span class="op">=</span> array(imgt.detach())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>tim <span class="op">=</span> transforms.ToPILImage()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>tim(imgc[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-51-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>正常来说用不到这个）</p>
</section>
</section>
<section id="池化层" class="level1">
<h1>池化层</h1>
<section id="nn.maxpool1d" class="level2">
<h2 class="anchored" data-anchor-id="nn.maxpool1d">nn.MaxPool1d</h2>
<p>最大池化</p>
<ul>
<li>kernel_size(int or tuple) - max pooling的窗口大小</li>
<li>stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size</li>
<li>padding(int or tuple, optional) - 输入的每一条边补充0的层数</li>
<li>dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数</li>
<li>return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助</li>
<li>ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
</ul>
</section>
<section id="nn.avgpool1d" class="level2">
<h2 class="anchored" data-anchor-id="nn.avgpool1d">nn.AvgPool1d</h2>
<p>平均池化</p>
</section>
<section id="nn.fractionalmaxpool2d" class="level2">
<h2 class="anchored" data-anchor-id="nn.fractionalmaxpool2d">nn.FractionalMaxPool2d</h2>
<p>分数最大化池化</p>
<ul>
<li>kernel_size(int or tuple) - 最大池化操作时的窗口大小。可以是一个数字（表示K<em>K的窗口），也可以是一个元组（kh</em>kw）</li>
<li>output_size - 输出图像的尺寸。可以使用一个tuple指定(oH,oW)，也可以使用一个数字oH指定一个oH*oH的输出。</li>
<li>output_ratio – 将输入图像的大小的百分比指定为输出图片的大小，使用一个范围在(0,1)之间的数字指定</li>
<li>return_indices - 默认值False，如果设置为True，会返回输出的索引，索引对 nn.MaxUnpool2d有用。</li>
</ul>
<p>也就是说，它可以对任意大小的东西进行最大池化，并输出相同/相同比例大小的图片</p>
<p>有点像最近邻采样</p>
</section>
<section id="nn.lppool2d" class="level2">
<h2 class="anchored" data-anchor-id="nn.lppool2d">nn.LPPool2d</h2>
<p>二维幂平均池化</p>
<p>也就是一个结合了最大池化和平均池化的方法</p>
<p><code>m = nn.LPPool2d(p, (3, 2), stride=(2, 1))</code><br>
- 当p为无穷大的时候时，等价于最大池化操作 - 当p=1时，等价于平均池化操作</p>
</section>
<section id="nn.adaptivemaxpool2d" class="level2">
<h2 class="anchored" data-anchor-id="nn.adaptivemaxpool2d">nn.AdaptiveMaxPool2d</h2>
<p>自适应最大池化</p>
<ul>
<li>output_size: 输出信号的尺寸,可以用（H,W）表示H<em>W的输出，也可以使用数字H表示H</em>H大小的输出</li>
<li>return_indices: 如果设置为True，会返回输出的索引。对 nn.MaxUnpool2d有用，默认值是False</li>
</ul>
</section>
</section>
<section id="non-linear-activations-非线性激活函数" class="level1">
<h1>Non-Linear Activations 非线性激活函数</h1>
<p>激活函数是用来增加网络非线性的</p>
<p>常用的有<code>relu</code>,<code>sigmoid</code>,<code>softmax</code></p>
<section id="torch.nn.relu" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn.relu">torch.nn.ReLU</h2>
<p>线性整流单元(也就是把小于零的变成0）<br>
<span class="math inline">\({ReLU}(x)= max(0, x)\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.ReLU()(x)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-52-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="nn.relu6" class="level2">
<h2 class="anchored" data-anchor-id="nn.relu6">nn.ReLU6</h2>
<p>对输入的每一个元素运用函数<br>
<span class="math inline">\({ReLU6}(x) = min(max(0,x), 6)\)</span></p>
<p>也就是把小于0的变成0,大于6的变成6</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.ReLU6()(x)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-53-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="nn.elu" class="level2">
<h2 class="anchored" data-anchor-id="nn.elu">nn.ELU</h2>
<p>对输入的每一个元素运用函数<br>
<span class="math inline">\(f(x) = max(0,x) + min(0, alpha * (e^x - 1))\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.ELU(<span class="dv">10</span>)(x)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-54-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>也就是对于0以下的，使用e**x，对于0以上的，使用普通的ReLU</p>
</section>
<section id="nn.prelu" class="level2">
<h2 class="anchored" data-anchor-id="nn.prelu">nn.PReLU</h2>
<p>对输入的每一个元素运用函数<span class="math inline">\(PReLU(x) = max(0,x) + a * min(0,x)\)</span></p>
<p>a是一个可学习参数。当没有声明时，nn.PReLU()在所有的输入中只有一个参数a；如果是nn.PReLU(nChannels)，a将应用到每个输入。</p>
<ul>
<li>num_parameters=1 -需要学习的a的个数<br>
</li>
<li>init=0.25 -a的默认值</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.PReLU(num_parameters<span class="op">=</span><span class="dv">1</span>, init<span class="op">=-</span><span class="dv">1</span>)(x).detach().numpy()</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-55-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>也就是说，这是一个在负值上进行y=ax，正值进行y=x的函数</p>
<p>不要对a使用权重衰减</p>
</section>
<section id="nn.leakyrelu" class="level2">
<h2 class="anchored" data-anchor-id="nn.leakyrelu">nn.LeakyReLU</h2>
<ul>
<li>negative_slope：控制负斜率的角度，默认等于0.01</li>
<li>inplace-选择是否进行覆盖运算</li>
</ul>
<p>对输入的每一个元素运用<span class="math inline">\(f(x) = max(0, x) + {negative_slope} * min(0, x)\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.LeakyReLU(<span class="op">-</span><span class="dv">1</span>)(x).detach().numpy()</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-56-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>与PReLU差不多，但是a固定</p>
</section>
<section id="nn.threshold" class="level2">
<h2 class="anchored" data-anchor-id="nn.threshold">nn.Threshold</h2>
<ul>
<li>threshold：阈值</li>
<li>value：输入值小于阈值则会被value代替</li>
<li>inplace：选择是否进行覆盖运算</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Threshold(threshold<span class="op">=</span><span class="dv">2</span>, value<span class="op">=</span><span class="dv">3</span>)(x).detach().numpy()</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-5"><a href="#cb73-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-57-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>相当于ReLU的泛化</p>
</section>
<section id="nn.hardtanh" class="level2">
<h2 class="anchored" data-anchor-id="nn.hardtanh">nn.Hardtanh</h2>
<ul>
<li>min_val：线性区域范围最小值</li>
<li>max_val：线性区域范围最大值</li>
<li>inplace：选择是否进行覆盖运算</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Hardtanh(min_val<span class="op">=-</span><span class="dv">1</span>, max_val<span class="op">=</span><span class="dv">2</span>)(x).detach().numpy()</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-58-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>虽然带有<code>tanh</code>,但和tan没什么关系。它就相当于<code>ReLU6</code>的泛化</p>
</section>
<section id="nn.sigmoid" class="level2">
<h2 class="anchored" data-anchor-id="nn.sigmoid">nn.Sigmoid</h2>
<p><span class="math display">\[f ( x ) = 1 / ( 1 + e − x )\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Sigmoid()(x).detach().numpy()</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-59-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="nn.tanh" class="level2">
<h2 class="anchored" data-anchor-id="nn.tanh">nn.Tanh</h2>
<p><span class="math display">\[f ( x ) = (e^x − e^x) / (e^x + e^x)\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Tanh()(x).detach().numpy()</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-60-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>其实和sigmoid挺像的，不过更窄一点</p>
</section>
<section id="nn.logsigmoid" class="level2">
<h2 class="anchored" data-anchor-id="nn.logsigmoid">nn.LogSigmoid</h2>
<p><span class="math display">\[LogSigmoid(x) = log( 1 / ( 1 + e^{-x}))\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.LogSigmoid()(x).detach().numpy()</span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-61-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>也就是<code>sigmoid</code>的对数</p>
</section>
<section id="nn.softplus" class="level2">
<h2 class="anchored" data-anchor-id="nn.softplus">nn.Softplus</h2>
<p><span class="math display">\[f ( x ) = \frac{1}{beta}∗log(1+e(beta∗xi))\]</span></p>
<ul>
<li>beta：Softplus函数的beta值(默认为1)</li>
<li>threshold：阈值(i)(默认为20）</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Softplus(beta<span class="op">=</span><span class="dv">1</span>, threshold<span class="op">=</span><span class="dv">20</span>)(x).detach().numpy()</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-62-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><code>Softplus</code>函数是<code>ReLU</code>函数的平滑逼近</p>
</section>
<section id="nn.softshrink" class="level2">
<h2 class="anchored" data-anchor-id="nn.softshrink">nn.Softshrink</h2>
<p><span class="math display">\[f ( x ) = x − lambda , if x &gt; lambda f ( x ) = x + lambda , if x &lt; −lambda f(x)=0,otherwise\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Softshrink(lambd<span class="op">=</span><span class="dv">6</span>)(x).detach().numpy()</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-63-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>如你所见，中间的台阶大小是12</p>
</section>
<section id="nn.softsign" class="level2">
<h2 class="anchored" data-anchor-id="nn.softsign">nn.Softsign</h2>
<p><span class="math display">\[f(x) = x / (1 + |x|)\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">100</span>, <span class="dv">100</span>, <span class="dv">1000</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Softsign()(x).detach().numpy()</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-64-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="nn.tanhshrink" class="level2">
<h2 class="anchored" data-anchor-id="nn.tanhshrink">nn.Tanhshrink</h2>
<p><span class="math display">\[Tanhshrink ( x ) = x − Tanh ( x )\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> nn.Tanhshrink()(x).detach().numpy()</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="22_12_23_torch.nn_files/figure-html/cell-65-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="nn.softmin" class="level2">
<h2 class="anchored" data-anchor-id="nn.softmin">nn.Softmin</h2>
<p>它对n维输入张量运用Softmin函数，将张量的每个元素缩放到（0,1）区间且和为1。</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.Softmin(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.arange(<span class="dv">0</span>, <span class="dv">20</span>).reshape(<span class="dv">5</span>, <span class="dv">4</span>).<span class="bu">float</span>())</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(input0)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(m(input0))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
tensor([[9.8168e-01, 9.8168e-01, 9.8168e-01, 9.8168e-01],
        [1.7980e-02, 1.7980e-02, 1.7980e-02, 1.7980e-02],
        [3.2932e-04, 3.2932e-04, 3.2932e-04, 3.2932e-04],
        [6.0317e-06, 6.0317e-06, 6.0317e-06, 6.0317e-06],
        [1.1047e-07, 1.1047e-07, 1.1047e-07, 1.1047e-07]])</code></pre>
</div>
</div>
</section>
<section id="nn.softmax" class="level2">
<h2 class="anchored" data-anchor-id="nn.softmax">nn.Softmax</h2>
<p>和<code>Softmin</code>”相反“</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.Softmax(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.arange(<span class="dv">0</span>, <span class="dv">20</span>).reshape(<span class="dv">5</span>, <span class="dv">4</span>).<span class="bu">float</span>())</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(input0)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(m(input0))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
tensor([[1.1047e-07, 1.1047e-07, 1.1047e-07, 1.1047e-07],
        [6.0317e-06, 6.0317e-06, 6.0317e-06, 6.0317e-06],
        [3.2932e-04, 3.2932e-04, 3.2932e-04, 3.2932e-04],
        [1.7980e-02, 1.7980e-02, 1.7980e-02, 1.7980e-02],
        [9.8168e-01, 9.8168e-01, 9.8168e-01, 9.8168e-01]])</code></pre>
</div>
</div>
</section>
<section id="nn.logsoftmax" class="level2">
<h2 class="anchored" data-anchor-id="nn.logsoftmax">nn.LogSoftmax</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.LogSoftmax(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.arange(<span class="dv">0</span>, <span class="dv">20</span>).reshape(<span class="dv">5</span>, <span class="dv">4</span>).<span class="bu">float</span>())</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(input0)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(m(input0))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.],
        [12., 13., 14., 15.],
        [16., 17., 18., 19.]])
tensor([[-16.0185, -16.0185, -16.0185, -16.0185],
        [-12.0185, -12.0185, -12.0185, -12.0185],
        [ -8.0185,  -8.0185,  -8.0185,  -8.0185],
        [ -4.0185,  -4.0185,  -4.0185,  -4.0185],
        [ -0.0185,  -0.0185,  -0.0185,  -0.0185]])</code></pre>
</div>
</div>
</section>
</section>
<section id="normalization-layers-标准化层" class="level1">
<h1>Normalization layers 标准化层</h1>
<section id="nn.batchnorm2d" class="level2">
<h2 class="anchored" data-anchor-id="nn.batchnorm2d">nn.BatchNorm2d</h2>
<p><em>num_features, eps=1e-05, momentum=0.1, affine=True</em></p>
<p>对小批量(mini-batch)3d数据组成的4d输入进行批标准化(Batch Normalization)操作</p>
<ul>
<li>num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features x height x width’</li>
<li>eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li>
<li>momentum： 动态均值和动态方差所使用的动量。默认为0.1。</li>
<li>affine： 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="co"># With Learnable Parameters</span></span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.BatchNorm2d(<span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Without Learnable Parameters</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.BatchNorm2d(<span class="dv">100</span>, affine<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.rand(<span class="dv">20</span>, <span class="dv">100</span>, <span class="dv">35</span>, <span class="dv">45</span>))</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> input0.mean()</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> input0.var()</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(std, var)</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> m(input0)</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> input0.mean()</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> input0.var()</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(std, var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(0.0833) tensor(0.9999, grad_fn=&lt;VarBackward0&gt;)
tensor(0.9999, grad_fn=&lt;VarBackward0&gt;) tensor(0.9999, grad_fn=&lt;VarBackward0&gt;)</code></pre>
</div>
</div>
<p>也就是把平均值变成1,标准差变成1</p>
</section>
</section>
<section id="recurrent-layers-循环层" class="level1">
<h1>Recurrent layers 循环层</h1>
<p>恭喜你看到了这里！这里是循环神经网络（RNN）</p>
<section id="nn.rnnargs-kwargs" class="level2">
<h2 class="anchored" data-anchor-id="nn.rnnargs-kwargs">nn.RNN(args, *kwargs)</h2>
<p><em>这只是一个几层全连接和激活函数组成的简单RNN而已……</em><br>
<em>它是层啊……那没事了</em></p>
<p>将一个多层的 <code>Elman RNN</code>，激活函数为<code>tanh</code>或者<code>ReLU</code>，用于输入序列。</p>
<p>对输入序列中每个元素，RNN每层的计算公式为 <span class="math display">\[ h_t=tanh(w_{ih} x_t+b_{ih}+w_{hh} h_{t-1}+b_{hh}) \]</span> <span class="math inline">\(h_t\)</span>是时刻<span class="math inline">\(t\)</span>的隐状态。 <span class="math inline">\(x_t\)</span>是上一层时刻<span class="math inline">\(t\)</span>的隐状态，或者是第一层在时刻<span class="math inline">\(t\)</span>的输入。如果nonlinearity=‘relu’,那么将使用relu代替tanh作为激活函数。</p>
<section id="参数" class="level3">
<h3 class="anchored" data-anchor-id="参数">参数</h3>
<ul>
<li><p>input_size – 输入x的特征数量。</p></li>
<li><p>hidden_size – 隐层的特征数量。</p></li>
<li><p>num_layers – RNN的层数。</p></li>
<li><p>nonlinearity – 指定非线性函数使用tanh还是relu。默认是tanh。</p></li>
<li><p>bias – 如果是False，那么RNN层就不会使用偏置权重 <span class="math inline">\(b_ih\)</span>和<span class="math inline">\(b_hh\)</span>,默认是True</p></li>
<li><p>batch_first – 如果True的话，那么输入Tensor的shape应该是[batch_size, time_step, feature],输出也是这样。</p></li>
<li><p>dropout – 如果值非零，那么除了最后一层外，其它层的输出都会套上一个dropout层。</p></li>
<li><p>bidirectional – 如果True，将会变成一个双向RNN，默认为False。</p></li>
</ul>
</section>
<section id="输入-input-h_0" class="level3">
<h3 class="anchored" data-anchor-id="输入-input-h_0">输入 (input, h_0)</h3>
<p>input(seq_len, batch, input_size): 保存输入序列特征的<code>tensor</code>。</p>
<p><code>input</code>可以是被填充的变长的序列。细节请看<code>torch.nn.utils.rnn.pack_padded_sequence()</code></p>
<ul>
<li>h_0(num_layers * num_directions, batch, hidden_size): 保存着初始隐状态的tensor</li>
</ul>
</section>
<section id="输出-output-h_n" class="level3">
<h3 class="anchored" data-anchor-id="输出-output-h_n">输出 (output, h_n)</h3>
<ul>
<li>output (seq_len, batch, hidden_size * num_directions): 保存着RNN最后一层的输出特征。如果输入是被填充过的序列，那么输出也是被填充的序列。</li>
<li>h_n (num_layers * num_directions, batch, hidden_size): 保存着最后一个时刻隐状态。</li>
</ul>
</section>
<section id="属性" class="level3">
<h3 class="anchored" data-anchor-id="属性">属性</h3>
<ul>
<li><p>weight_ih_l[k] – 第k层的 input-hidden 权重， 可学习，形状是(input_size x hidden_size)。</p></li>
<li><p>weight_hh_l[k] – 第k层的 hidden-hidden 权重， 可学习，形状是(hidden_size x hidden_size)</p></li>
<li><p>bias_ih_l[k] – 第k层的 input-hidden 偏置， 可学习，形状是(hidden_size)</p></li>
<li><p>bias_hh_l[k] – 第k层的 hidden-hidden 偏置， 可学习，形状是(hidden_size)</p></li>
</ul>
</section>
<section id="示例" class="level3">
<h3 class="anchored" data-anchor-id="示例">示例</h3>
<p>建立一个RNN</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="co">#rnn = nn.RNN(10, 30, 1)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> nn.RNN(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>生成假数据：</p>
<p>input0为时间序列<br>
h0为第一份输入</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.arange(<span class="dv">0</span>, <span class="dv">100</span>).reshape(<span class="dv">100</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>).<span class="bu">float</span>())</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>h0 <span class="op">=</span> input0[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>网络的内容</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>rnn.zero_grad()</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> rnn.parameters():</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(i.grad)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#i.data.add_(-1, i.grad.data)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.]])
tensor([[0.]])
tensor([0.])
tensor([0.])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>output, hn <span class="op">=</span> rnn(input0[<span class="dv">1</span>], h0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-1.0000]], grad_fn=&lt;SqueezeBackward1&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>input0[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1.]])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>hn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.4810]], grad_fn=&lt;SqueezeBackward1&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="nn.lstm" class="level2">
<h2 class="anchored" data-anchor-id="nn.lstm">nn.LSTM</h2>
<p><em>长短期记忆</em></p>
<p>将一个多层的 (LSTM) 应用到输入序列。</p>
<p>对输入序列的每个元素，LSTM的每层都会执行以下计算：</p>
<p><span class="math display">\[ \begin{aligned} i_t &amp;= sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}) \ f_t &amp;= sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf}) \ o_t &amp;= sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho})\ g_t &amp;= tanh(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg})\ c_t &amp;= f_tc_{t-1}+i_tg_t\ h_t &amp;= o_t*tanh(c_t) \end{aligned} \]</span></p>
<p><span class="math inline">\(h_t\)</span>是时刻<span class="math inline">\(t\)</span>的隐状态,<span class="math inline">\(c_t\)</span>是时刻<span class="math inline">\(t\)</span>的细胞状态，<span class="math inline">\(x_t\)</span>是上一层的在时刻<span class="math inline">\(t\)</span>的隐状态或者是第一层在时刻<span class="math inline">\(t\)</span>的输入。<span class="math inline">\(i_t, f_t, g_t, o_t\)</span> 分别代表 输入门，遗忘门，细胞和输出门。</p>
<p><em>其实这东西不看也没什么关系的</em></p>
<section id="参数-1" class="level3">
<h3 class="anchored" data-anchor-id="参数-1">参数</h3>
<ul>
<li><p>input_size – 输入的特征维度</p></li>
<li><p>hidden_size – 隐状态的特征维度</p></li>
<li><p>num_layers – 层数（和时序展开要区分开）</p></li>
<li><p>bias – 如果为False，那么LSTM将不会使用<span class="math inline">\(b_{ih},b_{hh}\)</span>，默认为True。</p></li>
<li><p>batch_first – 如果为True，那么输入和输出Tensor的形状为(batch, seq, feature)</p></li>
<li><p>dropout – 如果非零的话，将会在RNN的输出上加个dropout，最后一层除外。</p></li>
<li><p>bidirectional – 如果为True，将会变成一个双向RNN，默认为False。</p></li>
</ul>
</section>
<section id="输入-input-h_0-c_0" class="level3">
<h3 class="anchored" data-anchor-id="输入-input-h_0-c_0">输入: input, (h_0, c_0)</h3>
<ul>
<li><p>input (seq_len, batch, input_size): 包含输入序列特征的Tensor。也可以是packed variable。</p></li>
<li><p>h_0 (num_layers * num_directions, batch, hidden_size):保存着batch中每个元素的初始化隐状态的Tensor</p></li>
<li><p>c_0 (num_layers * num_directions, batch, hidden_size): 保存着batch中每个元素的初始化细胞状态的Tensor</p></li>
</ul>
</section>
<section id="输出output-h_n-c_n" class="level3">
<h3 class="anchored" data-anchor-id="输出output-h_n-c_n">输出:output, (h_n, c_n)</h3>
<ul>
<li><p>output (seq_len, batch, hidden_size * num_directions): 保存RNN最后一层的输出的Tensor。 如果输入是torch.nn.utils.rnn.PackedSequence，那么输出也是torch.nn.utils.rnn.PackedSequence。</p></li>
<li><p>h_n (num_layers * num_directions, batch, hidden_size): Tensor，保存着RNN最后一个时间步的隐状态。</p></li>
<li><p>c_n (num_layers * num_directions, batch, hidden_size): Tensor，保存着RNN最后一个时间步的细胞状态。</p></li>
</ul>
</section>
<section id="属性-1" class="level3">
<h3 class="anchored" data-anchor-id="属性-1">属性</h3>
<ul>
<li><p>weight_ih_l[k] – 第k层可学习的input-hidden权重(<span class="math inline">\(W_{ii}|W_{if}|W_{ig}|W_{io}\)</span>)，形状为(input_size x 4*hidden_size)</p></li>
<li><p>weight_hh_l[k] – 第k层可学习的hidden-hidden权重(<span class="math inline">\(W_{hi}|W_{hf}|W_{hg}|W_{ho}\)</span>)，形状为(hidden_size x 4*hidden_size)。</p></li>
<li><p>bias_ih_l[k] – 第k层可学习的input-hidden偏置(<span class="math inline">\(b_{ii}|b_{if}|b_{ig}|b_{io}\)</span>)，形状为( 4*hidden_size)</p></li>
<li><p>bias_hh_l[k] – 第k层可学习的hidden-hidden偏置(<span class="math inline">\(b_{hi}|b_{hf}|b_{hg}|b_{ho}\)</span>)，形状为( 4*hidden_size)。</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>lstm <span class="op">=</span> nn.LSTM(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">2</span>)</span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.randn(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">10</span>))</span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true" tabindex="-1"></a>h0 <span class="op">=</span> Variable(torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true" tabindex="-1"></a>c0 <span class="op">=</span> Variable(torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true" tabindex="-1"></a>output, hn <span class="op">=</span> lstm(input0, (h0, c0))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>output.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 3, 20])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a>hn[<span class="dv">0</span>].size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([2, 3, 20])</code></pre>
</div>
</div>
</section>
</section>
<section id="nn.gru" class="level2">
<h2 class="anchored" data-anchor-id="nn.gru">nn.GRU</h2>
<p><em>与LSTM相比，GRU内部少了一个”门控“，参数比LSTM少，但是却也能够达到与LSTM相当的功能。</em></p>
<p>将一个多层的GRU用于输入序列。</p>
<p>对输入序列中的每个元素，每层进行了一下计算：</p>
<p><span class="math display">\[ \begin{aligned} r_t&amp;=sigmoid(W_{ir}x_t+b_{ir}+W_{hr}h_{(t-1)}+b_{hr})\ i_t&amp;=sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{(t-1)}+b_{hi})\ n_t&amp;=tanh(W_{in}x_t+b_{in}+rt(W_{hn}h_{(t-1)}+b_{hn}))\ h_t&amp;=(1-i_t) nt+i_t*h(t-1) \end{aligned} \]</span> <span class="math inline">\(h_t\)</span>是是时间<span class="math inline">\(t\)</span>的上的隐状态，<span class="math inline">\(x_t\)</span>是前一层<span class="math inline">\(t\)</span>时刻的隐状态或者是第一层的<span class="math inline">\(t\)</span>时刻的输入，<span class="math inline">\(r_t, i_t, n_t\)</span>分别是重置门，输入门和新门。</p>
<section id="示例-1" class="level3">
<h3 class="anchored" data-anchor-id="示例-1">示例：</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> nn.GRU(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">2</span>)</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.randn(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">10</span>))</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a>h0 <span class="op">=</span> Variable(torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb109-4"><a href="#cb109-4" aria-hidden="true" tabindex="-1"></a>output, hn <span class="op">=</span> rnn(input0, h0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>output.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([5, 3, 20])</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>hn.size()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([2, 3, 20])</code></pre>
</div>
</div>
</section>
</section>
<section id="nn.rnncell" class="level2">
<h2 class="anchored" data-anchor-id="nn.rnncell">nn.RNNCell</h2>
<p>将一个多层的 <code>Elman RNNCell</code>，激活函数为<code>tanh</code>或者<code>ReLU</code>，应用于输入序列。公式： <span class="math display">\[ h'=tanh(w_{ih} x+b_{ih}+w_{hh} h+b_{hh}) \]</span> 如果nonlinearity=relu，那么将会使用ReLU来代替tanh。</p>
<section id="参数-2" class="level3">
<h3 class="anchored" data-anchor-id="参数-2">参数</h3>
<ul>
<li><p>input_size – 输入<span class="math inline">\(x\)</span>，特征的维度。</p></li>
<li><p>hidden_size – 隐状态特征的维度。</p></li>
<li><p>bias – 如果为False，RNN cell中将不会加入bias，默认为True。</p></li>
<li><p>nonlinearity – 用于选择非线性激活函数 [tanh|relu]. 默认值为： tanh</p></li>
</ul>
</section>
<section id="输入-input-hidden" class="level3">
<h3 class="anchored" data-anchor-id="输入-input-hidden">输入： input, hidden</h3>
<ul>
<li><p>input (batch, input_size): 包含输入特征的tensor。</p></li>
<li><p>hidden (batch, hidden_size): 保存着初始隐状态值的tensor。</p></li>
</ul>
</section>
<section id="输出-h" class="level3">
<h3 class="anchored" data-anchor-id="输出-h">输出： h’</h3>
<ul>
<li>h’ (batch, hidden_size):下一个时刻的隐状态。</li>
</ul>
</section>
<section id="属性-2" class="level3">
<h3 class="anchored" data-anchor-id="属性-2">属性：</h3>
<ul>
<li><p>weight_ih – input-hidden 权重， 可学习，形状是(input_size x hidden_size)。</p></li>
<li><p>weight_hh – hidden-hidden 权重， 可学习，形状是(hidden_size x hidden_size)</p></li>
<li><p>bias_ih – input-hidden 偏置， 可学习，形状是(hidden_size)</p></li>
<li><p>bias_hh – hidden-hidden 偏置， 可学习，形状是(hidden_size)</p></li>
</ul>
</section>
<section id="示例-2" class="level3">
<h3 class="anchored" data-anchor-id="示例-2">示例</h3>
<p>初始化一个（10,20）的rnncell（类似全连接层）</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> nn.RNNCell(<span class="dv">10</span>, <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>生成随机数据</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.randn(<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">10</span>))</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a>hx <span class="op">=</span> Variable(torch.randn(<span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>进行预测</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>):</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true" tabindex="-1"></a>    hx <span class="op">=</span> rnn(input0[i], hx)</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true" tabindex="-1"></a>    output.append(hx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>6</code></pre>
</div>
</div>
</section>
</section>
<section id="nn.lstmcell" class="level2">
<h2 class="anchored" data-anchor-id="nn.lstmcell">nn.LSTMCell</h2>
<p>公式：</p>
<p><span class="math display">\[ \begin{aligned} i &amp;= sigmoid(W_{ii}x+b_{ii}+W_{hi}h+b_{hi}) \ f &amp;= sigmoid(W_{if}x+b_{if}+W_{hf}h+b_{hf}) \ o &amp;= sigmoid(W_{io}x+b_{io}+W_{ho}h+b_{ho})\ g &amp;= tanh(W_{ig}x+b_{ig}+W_{hg}h+b_{hg})\ c' &amp;= f_tc_{t-1}+i_tg_t\ h' &amp;= o_t*tanh(c') \end{aligned} \]</span></p>
<section id="参数-3" class="level3">
<h3 class="anchored" data-anchor-id="参数-3">参数</h3>
<ul>
<li>input_size – 输入的特征维度。</li>
<li>hdden_size – 隐状态的维度。</li>
<li>bias – 如果为False，那么将不会使用bias。默认为True。</li>
</ul>
</section>
<section id="输入-input-h_0-c_0-1" class="level3">
<h3 class="anchored" data-anchor-id="输入-input-h_0-c_0-1">输入: input, (h_0, c_0)</h3>
<ul>
<li><p>input (seq_len, batch, input_size): 包含输入序列特征的Tensor。也可以是packed variable</p></li>
<li><p>h_0 ( batch, hidden_size):保存着batch中每个元素的初始化隐状态的Tensor</p></li>
<li><p>c_0 (batch, hidden_size): 保存着batch中每个元素的初始化细胞状态的Tensor</p></li>
</ul>
</section>
<section id="输出-h_1-c_1" class="level3">
<h3 class="anchored" data-anchor-id="输出-h_1-c_1">输出： h_1, c_1</h3>
<ul>
<li>h_1 (batch, hidden_size): 下一个时刻的隐状态。</li>
<li>c_1 (batch, hidden_size): 下一个时刻的细胞状态。</li>
</ul>
</section>
<section id="属性-3" class="level3">
<h3 class="anchored" data-anchor-id="属性-3">属性:</h3>
<ul>
<li><p>weight_ih – input-hidden权重(<span class="math inline">\(W_{ii}|W_{if}|W_{ig}|W_{io}\)</span>)，形状为(input_size x 4*hidden_size)</p></li>
<li><p>weight_hh – hidden-hidden权重(<span class="math inline">\(W_{hi}|W_{hf}|W_{hg}|W_{ho}\)</span>)，形状为(hidden_size x 4*hidden_size)。</p></li>
<li><p>bias_ih – input-hidden偏置(<span class="math inline">\(b_{ii}|b_{if}|b_{ig}|b_{io}\)</span>)，形状为( 4*hidden_size)</p></li>
<li><p>bias_hh – hidden-hidden偏置(<span class="math inline">\(b_{hi}|b_{hf}|b_{hg}|b_{ho}\)</span>)，形状为( 4*hidden_size)。</p></li>
</ul>
</section>
<section id="例子" class="level3">
<h3 class="anchored" data-anchor-id="例子">例子</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> nn.LSTMCell(<span class="dv">10</span>, <span class="dv">20</span>)</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.randn(<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">10</span>))</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>hx <span class="op">=</span> Variable(torch.randn(<span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>cx <span class="op">=</span> Variable(torch.randn(<span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> []</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>):</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>    hx, cx <span class="op">=</span> rnn(input0[i], (hx, cx))</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>    output.append(hx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="nn.grucell" class="level2">
<h2 class="anchored" data-anchor-id="nn.grucell">nn.GRUCell</h2>
<p>同上</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>rnn <span class="op">=</span> nn.GRUCell(<span class="dv">10</span>, <span class="dv">20</span>)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.randn(<span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">10</span>))</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>hx <span class="op">=</span> Variable(torch.randn(<span class="dv">3</span>, <span class="dv">20</span>))</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> []</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">6</span>):</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>    hx <span class="op">=</span> rnn(input0[i], hx)</span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>    output.append(hx)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="linear-layers-线性层" class="level1">
<h1>Linear layers 线性层</h1>
<section id="nn.linearin_features-out_features-biastrue" class="level2">
<h2 class="anchored" data-anchor-id="nn.linearin_features-out_features-biastrue">nn.Linear(in_features, out_features, bias=True)</h2>
<p>对输入数据做线性变换：<span class="math inline">\(y = Ax + b\)</span></p>
<section id="参数-4" class="level3">
<h3 class="anchored" data-anchor-id="参数-4">参数：</h3>
<ul>
<li>in_features - 每个输入样本的大小</li>
<li>out_features - 每个输出样本的大小</li>
<li>bias - 若设置为False，这层不会学习偏置。默认值：True</li>
</ul>
</section>
<section id="形状" class="level3">
<h3 class="anchored" data-anchor-id="形状">形状：</h3>
<ul>
<li>输入: (N,in_features)</li>
<li>输出： (N,out_features)</li>
</ul>
</section>
<section id="属性-4" class="level3">
<h3 class="anchored" data-anchor-id="属性-4">属性：</h3>
<ul>
<li>weight -形状为(out_features x in_features)的模块中可学习的权值</li>
<li>bias -形状为(out_features)的模块中可学习的偏置</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb121"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.Linear(<span class="dv">20</span>, <span class="dv">30</span>)</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.randn(<span class="dv">128</span>, <span class="dv">20</span>))</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> m(input0)</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.size())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([128, 30])</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="dropout-layers-丢弃层" class="level1">
<h1>Dropout layers 丢弃层</h1>
<section id="nn.dropout-nn.dropout2d3d" class="level2">
<h2 class="anchored" data-anchor-id="nn.dropout-nn.dropout2d3d">nn.Dropout / nn.Dropout2d,3d</h2>
<p>随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。</p>
<section id="参数-5" class="level3">
<h3 class="anchored" data-anchor-id="参数-5">参数：</h3>
<ul>
<li>p - 将元素置0的概率。默认值：0.5</li>
<li>in-place - 若设置为True，会在原地执行操作。默认值：False</li>
</ul>
</section>
<section id="形状-1" class="level3">
<h3 class="anchored" data-anchor-id="形状-1">形状：</h3>
<ul>
<li>输入： 任意。输入可以为任意形状。</li>
<li>输出： 相同。输出和输入形状相同。</li>
</ul>
</section>
<section id="例子-1" class="level3">
<h3 class="anchored" data-anchor-id="例子-1">例子：</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> torch.autograd.Variable(torch.randn(<span class="dv">20</span>, <span class="dv">16</span>))</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> m(input0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>input0.count_nonzero()<span class="op">-</span>output.count_nonzero()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(46)</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="sparse-layers-稀疏层词向量产生层" class="level1">
<h1>Sparse layers 稀疏层（？）词向量产生层（！）</h1>
<section id="nn.embedding" class="level3">
<h3 class="anchored" data-anchor-id="nn.embedding">nn.Embedding</h3>
<p>一个保存了固定字典和大小的简单查找表。</p>
<p>这个模块常用来保存词嵌入和用下标检索它们。模块的输入是一个下标的列表，输出是对应的词嵌入。</p>
</section>
<section id="参数-6" class="level3">
<h3 class="anchored" data-anchor-id="参数-6">参数：</h3>
<ul>
<li>num_embeddings (int) - 嵌入字典的大小</li>
<li>embedding_dim (int) - 每个嵌入向量的大小</li>
<li>padding_idx (int, optional) - 如果提供的话，输出遇到此下标时用零填充</li>
<li>max_norm (float, optional) - 如果提供的话，会重新归一化词嵌入，使它们的范数小于提供的值</li>
<li>norm_type (float, optional) - 对于max_norm选项计算p范数时的p</li>
<li>scale_grad_by_freq (boolean, optional) - 如果提供的话，会根据字典中单词频率缩放梯度</li>
</ul>
</section>
<section id="属性-5" class="level3">
<h3 class="anchored" data-anchor-id="属性-5">属性：</h3>
<ul>
<li>weight (Tensor) -形状为(num_embeddings, embedding_dim)的模块中可学习的权值</li>
</ul>
</section>
<section id="输入-longtensor-n-w" class="level3">
<h3 class="anchored" data-anchor-id="输入-longtensor-n-w">输入： LongTensor (N, W),</h3>
<ul>
<li>N = mini-batch,</li>
<li>W = 每个mini-batch中提取的下标数</li>
</ul>
</section>
<section id="输出-n-w-embedding_dim" class="level3">
<h3 class="anchored" data-anchor-id="输出-n-w-embedding_dim">输出： (N, W, embedding_dim)</h3>
</section>
<section id="示例-3" class="level3">
<h3 class="anchored" data-anchor-id="示例-3">示例</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># an Embedding module containing 10 tensors of size 3</span></span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(<span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="co"># a batch of 2 samples of 4 indices each</span></span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> Variable(torch.LongTensor([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">3</span>], [<span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>]]))</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true" tabindex="-1"></a>embedding(input0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[[-1.3053,  1.2748,  0.5904],
         [-1.3053,  1.2748,  0.5904],
         [-1.3053,  1.2748,  0.5904],
         [-1.3053,  1.2748,  0.5904],
         [ 1.7692, -1.4968, -1.1010]],

        [[-0.3840,  0.3105,  1.5848],
         [ 1.7692, -1.4968, -1.1010],
         [ 0.3168, -0.3723,  0.9616],
         [ 0.3168, -0.3723,  0.9616],
         [ 0.3168, -0.3723,  0.9616]]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example with padding_idx</span></span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(<span class="dv">6</span>, <span class="dv">3</span>, padding_idx<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> Variable(torch.LongTensor([[<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">5</span>]]))<span class="co">#由于这里最大的是5,所以它认为你有至少六个词</span></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true" tabindex="-1"></a>embedding(<span class="bu">input</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[[ 0.0000,  0.0000,  0.0000],
         [ 0.4923, -0.1930, -1.2295],
         [ 0.0000,  0.0000,  0.0000],
         [-0.4291,  0.7431, -0.2245]]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p>嵌入的字典尺寸……怎么计算的？</p>
<p>就是单词的总和……并且它以你最大的那个数而不是类别数发出错误</p>
</section>
</section>
<section id="distance-functions" class="level1">
<h1>Distance functions</h1>
<section id="nn.pairwisedistancep2-eps1e-06" class="level2">
<h2 class="anchored" data-anchor-id="nn.pairwisedistancep2-eps1e-06">nn.PairwiseDistance(p=2, eps=1e-06)</h2>
<p>按批计算向量v1, v2之间的距离：</p>
<section id="参数-7" class="level3">
<h3 class="anchored" data-anchor-id="参数-7">参数：</h3>
<ul>
<li>x (Tensor): 包含两个输入batch的张量</li>
<li>p (real): 范数次数，默认值：2</li>
</ul>
</section>
<section id="输入-nd其中d向量维数" class="level3">
<h3 class="anchored" data-anchor-id="输入-nd其中d向量维数">输入： (N,D)，其中D=向量维数</h3>
</section>
<section id="输出-n1" class="level3">
<h3 class="anchored" data-anchor-id="输出-n1">输出： (N,1)</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>pdist <span class="op">=</span> nn.PairwiseDistance(<span class="dv">2</span>)</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true" tabindex="-1"></a>input1 <span class="op">=</span> autograd.Variable(torch.randn(<span class="dv">5</span>, <span class="dv">128</span>))</span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true" tabindex="-1"></a>input2 <span class="op">=</span> autograd.Variable(torch.randn(<span class="dv">5</span>, <span class="dv">128</span>))</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> pdist(input1, input2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([15.8720, 17.3040, 16.8190, 16.7946, 16.5825])</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="loss-functions-损失函数" class="level1">
<h1>Loss functions 损失函数</h1>
<p>基本用法：</p>
<pre><code>criterion = LossCriterion() #构造函数有自己的参数
loss = criterion(x, y) #调用标准时也有参数 </code></pre>
<section id="nn.l1loss" class="level2">
<h2 class="anchored" data-anchor-id="nn.l1loss">nn.L1Loss</h2>
<p>创建一个衡量输入x(模型预测输出)和目标y之间差的绝对值的平均值的标准。</p>
<p><span class="math display">\[loss(x,y)=1/n∑|xi−yi|\]</span></p>
<ul>
<li><p>x 和 y 可以是任意形状，每个包含n个元素。</p></li>
<li><p>对n个元素对应的差值的绝对值求和，得出来的结果除以n。</p></li>
<li><p>如果在创建L1Loss实例的时候在构造函数中传入size_average=False，那么求出来的绝对值的和将不会除以n</p></li>
</ul>
</section>
<section id="nn.mseloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.mseloss">nn.MSELoss</h2>
<p>创建一个衡量输入x(模型预测输出)和目标y之间均方误差标准。</p>
<p><span class="math display">\[loss(x,y)=1/n∑(xi−yi)^2\]</span></p>
<ul>
<li><p>x 和 y 可以是任意形状，每个包含n个元素。</p></li>
<li><p>对n个元素对应的差值的绝对值求和，得出来的结果除以n。</p></li>
<li><p>如果在创建MSELoss实例的时候在构造函数中传入size_average=False，那么求出来的平方和将不会除以n</p></li>
</ul>
</section>
<section id="nn.crossentropyloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.crossentropyloss">nn.CrossEntropyLoss</h2>
<p><em>交叉熵损失</em></p>
<p>此标准将LogSoftMax和NLLLoss集成到一个类中。</p>
<p>当训练一个多类分类器的时候，这个方法是十分有用的。</p>
<ul>
<li>weight(tensor): 1-D tensor，n个元素，分别代表n类的权重，如果你的训练样本很不均衡的话，是非常有用的。默认值为None。</li>
</ul>
<section id="调用时参数" class="level3">
<h3 class="anchored" data-anchor-id="调用时参数">调用时参数：</h3>
<ul>
<li><p>input : 包含每个类的得分，2-D tensor,shape为 batch*n</p></li>
<li><p>target: 大小为 n 的 1—D tensor，包含类别的索引(0到 n-1)。</p></li>
<li><p>计算出的loss对mini-batch的大小取了平均。</p></li>
</ul>
</section>
<section id="形状shape" class="level3">
<h3 class="anchored" data-anchor-id="形状shape">形状(shape)：</h3>
<ul>
<li><p>Input: (N,C) C 是类别的数量</p></li>
<li><p>Target: (N) N是mini-batch的大小，0 &lt;= targets[i] &lt;= C-1</p></li>
</ul>
</section>
</section>
<section id="nn.nllloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.nllloss">nn.NLLLoss</h2>
<p><em>负对数似然</em></p>
<p>用于训练一个n类分类器。</p>
<p><code>weight</code>参数应该是一个1-Dtensor(如果提供的话)，里面的值对应类别的权重。当你的训练集样本不均衡的话，使用这个参数是非常有用的。</p>
<p>输入是一个包含类别<code>log-probabilities</code>的2-D tensor，形状是（mini-batch， n）</p>
<p>可以通过在最后一层加<code>LogSoftmax</code>来获得类别的<code>log-probabilities</code>。</p>
<p>如果您不想增加一个额外层的话，您可以使用<code>CrossEntropyLoss</code>。</p>
<p>此loss期望的<code>target</code>是类别的索引 (0 to N-1, where N = number of classes)</p>
<section id="参数-8" class="level3">
<h3 class="anchored" data-anchor-id="参数-8">参数：</h3>
<ul>
<li><p>weight (Tensor, optional) – 手动指定每个类别的权重。如果给定的话，必须是长度为nclasses</p></li>
<li><p>size_average (bool, optional) – 默认情况下，会计算<code>mini-batch</code> <code>loss</code>的平均值。然而，如果size_average=False那么将会把mini-batch中所有样本的loss累加起来。</p></li>
</ul>
</section>
<section id="形状-2" class="level3">
<h3 class="anchored" data-anchor-id="形状-2">形状:</h3>
<ul>
<li><p>Input: (N,C) , C是类别的个数</p></li>
<li><p>Target: (N) ， target中每个值的大小满足 0 &lt;= targets[i] &lt;= C-1</p></li>
</ul>
</section>
<section id="例子-2" class="level3">
<h3 class="anchored" data-anchor-id="例子-2">例子：</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> nn.LogSoftmax(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.NLLLoss()</span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true" tabindex="-1"></a><span class="co"># input is of size nBatch x nClasses = 3 x 5</span></span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true" tabindex="-1"></a>input0 <span class="op">=</span> autograd.Variable(torch.randn(<span class="dv">3</span>, <span class="dv">5</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb134-5"><a href="#cb134-5" aria-hidden="true" tabindex="-1"></a><span class="co"># each element in target has to have 0 &lt;= value &lt; nclasses</span></span>
<span id="cb134-6"><a href="#cb134-6" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> autograd.Variable(torch.LongTensor([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>]))</span>
<span id="cb134-7"><a href="#cb134-7" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> loss(m(input0), target)</span>
<span id="cb134-8"><a href="#cb134-8" aria-hidden="true" tabindex="-1"></a>output.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="nn.nllloss2d" class="level3">
<h3 class="anchored" data-anchor-id="nn.nllloss2d">nn.NLLLoss2d</h3>
<ul>
<li><p>weight (Tensor, optional) – 用来作为每类的权重(如果提供的话),必须为1-Dtensor，大小为C：类别的个数。</p></li>
<li><p>size_average – 默认情况下，会计算 mini-batch loss均值。如果设置为 False 的话，将会累加mini-batch中所有样本的loss值。默认值：True。</p></li>
</ul>
</section>
<section id="nn.kldivloss" class="level3">
<h3 class="anchored" data-anchor-id="nn.kldivloss">nn.KLDivLoss</h3>
<p><a href="https://zh.wikipedia.org/zh-cn/%E7%9B%B8%E5%AF%B9%E7%86%B5"><em>相对熵损失</em></a></p>
<p>相对熵 = 某个策略的交叉熵 - 信息熵</p>
<p>计算 KL 散度损失。</p>
<p>KL散度常用来描述两个分布的距离，并在输出分布的空间上执行直接回归是有用的。</p>
<p>与<code>NLLLoss</code>一样，给定的输入应该是<code>log-probabilities</code>。然而。和<code>NLLLoss</code>不同的是，<code>input</code>不限于2-D tensor，因为此标准是基于element的。</p>
<p><code>target</code> 应该和 <code>input</code>的形状相同。</p>
<p>默认情况下，loss会基于element求平均。如果 size_average=False,loss 会被累加起来。</p>
</section>
</section>
<section id="nn.bceloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.bceloss">nn.BCELoss</h2>
<p>计算 target 与 output 之间的二元交叉熵。</p>
<p><span class="math display">\[ loss(o,t)=-\frac{1}{n}\sum_i(t[i] log(o[i])+(1-t[i]) log(1-o[i])) \]</span></p>
<p>如果weight被指定 ：</p>
<p><span class="math display">\[ loss(o,t)=-\frac{1}{n}\sum_iweights[i] (t[i] log(o[i])+(1-t[i])* log(1-o[i])) \]</span></p>
</section>
<section id="nn.marginrankingloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.marginrankingloss">nn.MarginRankingLoss</h2>
<p>排序损失函数</p>
<p>创建一个标准，给定输入 <span class="math inline">\(x1\)</span>,<span class="math inline">\(x2\)</span>两个1-D mini-batch Tensor’s，和一个<span class="math inline">\(y\)</span>(1-D mini-batch tensor) ,<span class="math inline">\(y\)</span>里面的值只能是-1或1。</p>
<p>如果 y=1，代表第一个输入的值应该大于第二个输入的值，如果y=-1的话，则相反。</p>
</section>
<section id="nn.hingeembeddingloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.hingeembeddingloss">nn.HingeEmbeddingLoss</h2>
<p>用于判断两个向量是否相似，输入是两个向量之间的距离</p>
<p>给定一个输入 <span class="math inline">\(x\)</span>(2-D mini-batch tensor)和对应的 标签 <span class="math inline">\(y\)</span> (1-D tensor,1,-1)，此函数用来计算两组向量之间的损失值。这个loss通常用来测量两个输入是否相似，即：使用L1 成对距离。典型是用在学习非线性 embedding或者半监督学习中：</p>
<p><span class="math inline">\(x\)</span>和<span class="math inline">\(y\)</span>可以是任意形状，且都有n的元素，loss的求和操作作用在所有的元素上，然后除以n。如果您不想除以n的话，可以通过设置size_average=False。</p>
<section id="例子-3" class="level3">
<h3 class="anchored" data-anchor-id="例子-3">例子：</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>hinge_loss <span class="op">=</span> nn.HingeEmbeddingLoss(margin<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">128</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">128</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> torch.cosine_similarity(a, b)</span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 定义a与b之间的距离为x</span></span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.size())</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> torch.empty(<span class="dv">100</span>).random_(<span class="dv">2</span>) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb135-8"><a href="#cb135-8" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> hinge_loss(x, y)</span>
<span id="cb135-9"><a href="#cb135-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output.item())</span>
<span id="cb135-10"><a href="#cb135-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-11"><a href="#cb135-11" aria-hidden="true" tabindex="-1"></a>hinge_loss <span class="op">=</span> nn.HingeEmbeddingLoss(margin<span class="op">=</span><span class="fl">0.2</span>, reduction<span class="op">=</span><span class="st">"none"</span>)</span>
<span id="cb135-12"><a href="#cb135-12" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> hinge_loss(x, y)</span>
<span id="cb135-13"><a href="#cb135-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([100])
0.4487411081790924
tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.9112, 0.0000, 0.9974, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0311, 0.0000, 0.0000, 0.0000, 1.0199, 0.0000, 0.9601, 0.0000, 1.0657, 0.8382, 1.0400, 1.0655,
        0.0000, 0.8794, 0.0000, 1.0720, 1.0367, 1.0841, 0.8692, 1.0141, 0.0000, 0.9980, 0.0000, 0.9341, 1.0703, 1.0783, 1.1506, 0.0000, 0.8374, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1050,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.1145, 1.0173, 0.0000, 0.0000, 0.9589, 0.0000, 1.0414, 0.8126, 0.8562, 0.0000, 1.2132, 1.0313, 0.0000, 0.0000, 1.0011, 0.0000, 0.0000, 1.1065,
        0.0000, 1.0584, 0.0000, 1.1716, 0.0000, 0.9385, 0.0000, 0.9754, 0.0000, 0.0000, 1.0354, 0.0000, 1.0717, 0.0000, 0.0000, 1.1551, 0.0000, 0.8999, 0.0000, 1.1494, 0.0000, 0.0000, 1.0500, 0.0000,
        0.0000, 0.9993, 0.0000, 1.1581], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
</section>
</section>
<section id="nn.multilabelmarginloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.multilabelmarginloss">nn.MultiLabelMarginLoss</h2>
<p>计算多标签分类的 hinge loss(margin-based loss) ，计算loss时需要两个输入： input x(2-D mini-batch Tensor)，和 output y(2-D tensor表示mini-batch中样本类别的索引)。</p>
<section id="例子-4" class="level3">
<h3 class="anchored" data-anchor-id="例子-4">例子：</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.FloatTensor([[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.8</span>], [<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.8</span>]])</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.size())</span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.LongTensor([<span class="dv">3</span>, <span class="dv">3</span>])</span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.size())</span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.MultiMarginLoss(reduction<span class="op">=</span><span class="st">"none"</span>)</span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true" tabindex="-1"></a>loss_val <span class="op">=</span> loss(x, y)</span>
<span id="cb137-8"><a href="#cb137-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss_val)</span>
<span id="cb137-9"><a href="#cb137-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb137-10"><a href="#cb137-10" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.MultiMarginLoss(reduction<span class="op">=</span><span class="st">"sum"</span>)</span>
<span id="cb137-11"><a href="#cb137-11" aria-hidden="true" tabindex="-1"></a>loss_val <span class="op">=</span> loss(x, y)</span>
<span id="cb137-12"><a href="#cb137-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss_val.item())</span>
<span id="cb137-13"><a href="#cb137-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(loss_val.item() <span class="op">/</span> x.size(<span class="dv">0</span>))</span>
<span id="cb137-14"><a href="#cb137-14" aria-hidden="true" tabindex="-1"></a><span class="co">#验证</span></span>
<span id="cb137-15"><a href="#cb137-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span> <span class="op">*</span> <span class="dv">1</span> <span class="op">/</span> <span class="dv">4</span> <span class="op">*</span> ((<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">+</span> <span class="fl">0.1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">+</span> <span class="fl">0.2</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">+</span> <span class="fl">0.4</span>) <span class="op">+</span></span>
<span id="cb137-16"><a href="#cb137-16" aria-hidden="true" tabindex="-1"></a>                       (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">+</span> <span class="fl">0.1</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">+</span> <span class="fl">0.2</span>) <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="fl">0.8</span> <span class="op">+</span> <span class="fl">0.4</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2, 4])
torch.Size([2])
tensor([0.3250, 0.3250])
0.6499999761581421
0.32499998807907104
0.32499999999999996</code></pre>
</div>
</div>
</section>
</section>
<section id="nn.smoothl1loss" class="level2">
<h2 class="anchored" data-anchor-id="nn.smoothl1loss">nn.SmoothL1Loss</h2>
<p>平滑版L1 loss。</p>
<p>此loss对于异常点的敏感性不如MSELoss，而且，在某些情况下防止了梯度爆炸，(参照 Fast R-CNN)。这个loss有时也被称为 Huber loss。</p>
</section>
<section id="nn.softmarginloss" class="level2">
<h2 class="anchored" data-anchor-id="nn.softmarginloss">nn.SoftMarginLoss</h2>
<p>用于二分类任务</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>